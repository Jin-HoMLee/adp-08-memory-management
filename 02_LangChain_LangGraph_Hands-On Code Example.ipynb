{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7364143e",
   "metadata": {},
   "source": [
    "# Hands-on code: Memory Management in LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428add",
   "metadata": {},
   "source": [
    "> Adapted and modified from https://docs.google.com/document/d/1asVTObtzIye0I9ypAztaeeI_sr_Hx2TORE02uUuqH_c/edit?tab=t.0#heading=h.jv3rycbe9ib6\n",
    "> \n",
    "> Di 21 Okt 2025 14:43:02 BST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13b2e7",
   "metadata": {},
   "source": [
    "---\n",
    "**Notes:**\n",
    "\n",
    "The whole notebook only runs when using a langchain version before 1.0.0, e.g. \n",
    "\n",
    "```bash\n",
    "langchain==0.3.27\n",
    "```\n",
    "\n",
    "and the strictly interdependent versions of \n",
    "\n",
    "```bash \n",
    "langchain-community==0.3.31\n",
    "langchain-openai==0.3.35\n",
    "langgraph==1.0.1\n",
    "````\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03923500",
   "metadata": {},
   "source": [
    "In LangChain and LangGraph, Memory is a critical component for creating intelligent and natural-feeling conversational applications. It allows an AI agent to remember information from past interactions, learn from feedback, and adapt to user preferences. LangChain's memory feature provides the foundation for this by referencing a stored history to enrich current prompts and then recording the latest exchange for future use. As agents handle more complex tasks, this capability becomes essential for both efficiency and user satisfaction.\n",
    "\n",
    "**Short-Term Memory:** This is thread-scoped, meaning it tracks the ongoing conversation within a single session or thread. It provides immediate context, but a full history can challenge an LLM's context window, potentially leading to errors or poor performance. LangGraph manages short-term memory as part of the agent's state, which is persisted via a checkpointer, allowing a thread to be resumed at any time.\n",
    "\n",
    "**Long-Term Memory:** This stores user-specific or application-level data across sessions and is shared between conversational threads. It is saved in custom \"namespaces\" and can be recalled at any time in any thread. LangGraph provides stores to save and recall long-term memories, enabling agents to retain knowledge indefinitely.\n",
    "\n",
    "LangChain provides several tools for managing conversation history, ranging from manual control to automated integration within chains.\n",
    "\n",
    "**ChatMessageHistory:** Manual Memory Management. For direct and simple control over a conversation's history outside of a formal chain, the ChatMessageHistory class is ideal. It allows for the manual tracking of dialogue exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3329d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"I'm heading to New York next week.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Great! It's a fantastic city.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "# ORIGINAL CODE\n",
    "# from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# Initialize the history object\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add user and AI messages\n",
    "history.add_user_message(\"I'm heading to New York next week.\")\n",
    "history.add_ai_message(\"Great! It's a fantastic city.\")\n",
    "\n",
    "# Access the list of messages\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100ae70",
   "metadata": {},
   "source": [
    "---\n",
    "**Notes:**\n",
    "\n",
    "Above code works in `langchain==0.3.27` \n",
    "\n",
    "In recent versions of LangChain >=1.0.0, many integrations and optional components — including in-memory chat history — were moved to the `langchain_community` package. The `langchain_community` namespace now contains community-maintained and non-core modules, such as `ChatMessageHistory`. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6826a",
   "metadata": {},
   "source": [
    "**ConversationBufferMemory: Automated Memory for Chains.** For integrating memory directly into chains, ConversationBufferMemory is a common choice. It holds a buffer of the conversation and makes it available to your prompt. Its behavior can be customized with two key parameters:\n",
    "- memory_key: A string that specifies the variable name in your prompt that will hold the chat history. It defaults to \"history\".\n",
    "- return_messages: A boolean that dictates the format of the history.\n",
    "    - If False (the default), it returns a single formatted string, which is ideal for standard LLMs.\n",
    "    - If True, it returns a list of message objects, which is the recommended format for Chat Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3da285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: What's the weather like?\\nAI: It's sunny today.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/97nfvbms25lb47qpfvgmgfc40000gn/T/ipykernel_29428/3882697118.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Save a conversation turn\n",
    "memory.save_context({\"input\": \"What's the weather like?\"}, {\"output\": \"It's sunny today.\"})\n",
    "\n",
    "# Load the memory as a string\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274376d",
   "metadata": {},
   "source": [
    "Integrating this memory into an LLMChain allows the model to access the conversation's history and provide contextually relevant responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ee010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/97nfvbms25lb47qpfvgmgfc40000gn/T/ipykernel_29428/4063513926.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great! Where are you looking to travel to and when? I can help you find the best flight options and prices.\n",
      " Nice to meet you, Sam! Is there a specific destination or date you have in mind for your trip? I can assist you in finding the perfect flight for your needs.\n",
      " Your name is Sam.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. Define LLM and Prompt\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"You are a helpful travel agent.\n",
    "\n",
    "Previous conversation:\n",
    "{history}\n",
    "\n",
    "New question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Configure Memory\n",
    "# The memory_key \"history\" matches the variable in the prompt\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "# 3. Build the Chain\n",
    "conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# 4. Run the Conversation\n",
    "response = conversation.predict(question=\"I want to book a flight.\")\n",
    "print(response)\n",
    "response = conversation.predict(question=\"My name is Sam, by the way.\")\n",
    "print(response)\n",
    "response = conversation.predict(question=\"What was my name again?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795219c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "The above code needs an API Key set, e.g. by providing an `OPENAI_API_KEY` in `.env`. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88624",
   "metadata": {},
   "source": [
    "For improved effectiveness with chat models, it is recommended to use a structured list of message objects by setting `return_messages=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca62cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jane! It's nice to meet you. How can I assist you today?\n",
      "Yes, I remember you, Jane. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import (\n",
    "   ChatPromptTemplate,\n",
    "   MessagesPlaceholder,\n",
    "   SystemMessagePromptTemplate,\n",
    "   HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# 1. Define Chat Model and Prompt\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate(\n",
    "   messages=[\n",
    "       SystemMessagePromptTemplate.from_template(\"You are a friendly assistant.\"),\n",
    "       MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "       HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "   ]\n",
    ")\n",
    "\n",
    "# 2. Configure Memory\n",
    "# return_messages=True is essential for chat models\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 3. Build the Chain\n",
    "conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# 4. Run the Conversation\n",
    "response = conversation.predict(question=\"Hi, I'm Jane.\")\n",
    "print(response)\n",
    "response = conversation.predict(question=\"Do you remember my name?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395ffed",
   "metadata": {},
   "source": [
    "**Types of Long-Term Memory:** Long-term memory allows systems to retain information across different conversations, providing a deeper level of context and personalization. It can be broken down into three types analogous to human memory:\n",
    "\n",
    "- **Semantic Memory: Remembering Facts:** This involves retaining specific facts and concepts, such as user preferences or domain knowledge. It is used to ground an agent's responses, leading to more personalized and relevant interactions. This information can be managed as a continuously updated user \"profile\" (a JSON document) or as a \"collection\" of individual factual documents.\n",
    "\n",
    "- **Episodic Memory: Remembering Experiences:** This involves recalling past events or actions. For AI agents, episodic memory is often used to remember how to accomplish a task. In practice, it's frequently implemented through few-shot example prompting, where an agent learns from past successful interaction sequences to perform tasks correctly.\n",
    "\n",
    "- **Procedural Memory: Remembering Rules:**  This is the memory of how to perform tasks—the agent's core instructions and behaviors, often contained in its system prompt. It's common for agents to modify their own prompts to adapt and improve. An effective technique is \"Reflection,\" where an agent is prompted with its current instructions and recent interactions, then asked to refine its own instructions.\n",
    "\n",
    "Below is pseudo-code demonstrating how an agent might use reflection to update its procedural memory stored in a LangGraph BaseStore\n",
    "\n",
    "```python\n",
    "# Node that updates the agent's instructions\n",
    "def update_instructions(state: State, store: BaseStore):\n",
    "   namespace = (\"instructions\",)\n",
    "   # Get the current instructions from the store\n",
    "   current_instructions = store.search(namespace)[0]\n",
    "  \n",
    "   # Create a prompt to ask the LLM to reflect on the conversation\n",
    "   # and generate new, improved instructions\n",
    "   prompt = prompt_template.format(\n",
    "       instructions=current_instructions.value[\"instructions\"],\n",
    "       conversation=state[\"messages\"]\n",
    "   )\n",
    "  \n",
    "   # Get the new instructions from the LLM\n",
    "   output = llm.invoke(prompt)\n",
    "   new_instructions = output['new_instructions']\n",
    "  \n",
    "   # Save the updated instructions back to the store\n",
    "   store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n",
    "\n",
    "# Node that uses the instructions to generate a response\n",
    "def call_model(state: State, store: BaseStore):\n",
    "   namespace = (\"agent_instructions\", )\n",
    "   # Retrieve the latest instructions from the store\n",
    "   instructions = store.get(namespace, key=\"agent_a\")[0]\n",
    "  \n",
    "   # Use the retrieved instructions to format the prompt\n",
    "   prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n",
    "   # ... application logic continues\n",
    "```\n",
    "\n",
    "LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom namespace (like a folder) and a distinct key (like a filename). This hierarchical structure allows for easy organization and retrieval of information. The following code demonstrates how to use InMemoryStore to put, get, and search for memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cea829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Item: Item(namespace=['my-user', 'chitchat'], key='a-memory', value={'rules': ['User likes short, direct language', 'User only speaks English & python'], 'my-key': 'my-value'}, created_at='2025-10-21T15:48:12.324622+00:00', updated_at='2025-10-21T15:48:12.324626+00:00')\n",
      "Search Results: [Item(namespace=['my-user', 'chitchat'], key='a-memory', value={'rules': ['User likes short, direct language', 'User only speaks English & python'], 'my-key': 'my-value'}, created_at='2025-10-21T15:48:12.324622+00:00', updated_at='2025-10-21T15:48:12.324626+00:00', score=0.9999999999999998)]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# A placeholder for a real embedding function\n",
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "   # In a real application, use a proper embedding model\n",
    "   return [[1.0, 2.0] for _ in texts]\n",
    "\n",
    "# Initialize an in-memory store. For production, use a database-backed store.\n",
    "store = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\n",
    "\n",
    "# Define a namespace for a specific user and application context\n",
    "user_id = \"my-user\"\n",
    "application_context = \"chitchat\"\n",
    "namespace = (user_id, application_context)\n",
    "\n",
    "# 1. Put a memory into the store\n",
    "store.put(\n",
    "   namespace,\n",
    "   \"a-memory\",  # The key for this memory\n",
    "   {\n",
    "       \"rules\": [\n",
    "           \"User likes short, direct language\",\n",
    "           \"User only speaks English & python\",\n",
    "       ],\n",
    "       \"my-key\": \"my-value\",\n",
    "   },\n",
    ")\n",
    "\n",
    "# 2. Get the memory by its namespace and key\n",
    "item = store.get(namespace, \"a-memory\")\n",
    "print(\"Retrieved Item:\", item)\n",
    "\n",
    "# 3. Search for memories within the namespace, filtering by content\n",
    "# and sorting by vector similarity to the query.\n",
    "items = store.search(\n",
    "   namespace,\n",
    "   filter={\"my-key\": \"my-value\"},\n",
    "   query=\"language preferences\"\n",
    ")\n",
    "print(\"Search Results:\", items)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
